{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd457a6f-f21a-485c-b63a-4a58662af4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, classification_report, confusion_matrix\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torchmetrics import AUROC\n",
    "import time\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import math \n",
    "import sys\n",
    "import gensim\n",
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "from gensim.models.phrases import Phraser, Phrases, ENGLISH_CONNECTOR_WORDS\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, make_scorer, precision_score, recall_score, f1_score\n",
    "\n",
    "'''\n",
    "Sets random seeds to ensure reproducibility across different runs.\n",
    "'''\n",
    "random_seed = 101 # or any of your favorite number \n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(device)\n",
    "\n",
    "\"\"\"\n",
    "Script for Data Preprocessing and Tokenizer Initialization\n",
    "\n",
    "This script reads an Excel file, splits the dataset into training, validation, and test sets, and initializes custom Word2Vec model.\n",
    "\n",
    "Data Structure:\n",
    "    - The target variable (label) for prediction is stored in a column named 'output' with 3 classes.\n",
    "    - The first feature column is clean-up operative notes.\n",
    "\n",
    "Functionalities:\n",
    "    1. Reads the Excel file into a Pandas DataFrame.\n",
    "    2. Splits the DataFrame into feature and label sets.\n",
    "    3. Further splits these into training, validation, and test sets.\n",
    "    4. Outputs the shapes of these datasets for verification.\n",
    "    5. Separates dynamic features from static features.\n",
    "    6. Initializes a custom tokenizer for further processing.\n",
    "    7. Tokenize the notes and create training, validation, and testing dataloaders.\n",
    "    8. Input dimensions are [512, 75, 1].\n",
    "    9. The first dimension represents the encoded input IDs for the custom Word2Vec model.\n",
    "    10. The third dimension represents the structured EHR data.\n",
    "    11. The fourth dimension represents the outcome labels.\n",
    "\"\"\"\n",
    "\n",
    "# Read the Excel file into a Pandas DataFrame\n",
    "df = pd.read_excel('glaucoma_surgery_dataset.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# Isolate the target variable (label) which we want to predict\n",
    "labels = df['output']\n",
    "# Remove the target variable from the feature set; axis=1 means we drop a column not a row\n",
    "features = df.drop('output', axis=1)\n",
    "\n",
    "# Split the data into a training set and a temporary validation/test set\n",
    "# We're using 30% of the data for the temporary validation/test set, stratified by the label\n",
    "train_features, val_test_features, train_labels, val_test_labels = train_test_split(features, labels, test_size=0.3, random_state=42, stratify=labels)\n",
    "\n",
    "# Further split the temporary validation/test set into validation and test sets\n",
    "# 2/3 of the data goes to the test set, stratified by the label\n",
    "val_features, test_features, val_labels, test_labels = train_test_split(val_test_features, val_test_labels, test_size=(2/3), random_state=42, stratify=val_test_labels)\n",
    "\n",
    "# Display shapes to ensure everything is as expected\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Val Features Shape:', val_features.shape)\n",
    "print('Val Labels Shape:', val_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)\n",
    "\n",
    "# Select static features by dropping the first column from the train, validation and test feature sets\n",
    "train_static = train_features.iloc[:, 1:]\n",
    "val_static = val_features.iloc[:, 1:]\n",
    "test_static = test_features.iloc[:, 1:]\n",
    "\n",
    "# Load the pre-trained Word2Vec model\n",
    "word2vec = Word2Vec.load(\"word2vec_D50_May.model\")\n",
    "\n",
    "# Set the size of the word embeddings\n",
    "n_embedding = 50\n",
    "\n",
    "#Tokenize the operative notes using custom Word2Vec model.\n",
    "\n",
    "#Generate the dataset and dataloader using TensorDataset and DataLoader from PyTorch.\n",
    "\n",
    "\"\"\"Start to train the model\"\"\"\n",
    "\n",
    "# Setting up hyperparameters and initial configurations\n",
    "batchSize = 16  # Batch size for training\n",
    "epochs = 200 # Number of epochs to train for\n",
    "LEARNING_RATE = 4e-5  # Learning rate for the optimizer\n",
    "WEIGHT_DECAY = 1e-5  # Weight decay factor for L2 regularization\n",
    "\n",
    "# Initialize class weights and move to GPU\n",
    "# These weights are useful if the dataset is imbalanced.\n",
    "class_weights = torch.tensor([0.2584, 0.8678, 0.8737]).to(device)  \n",
    "\n",
    "# Loss Function: Cross Entropy Loss with class weights\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Optimizer: Adam optimizer with learning rate and weight decay\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Ground truth labels for the validation set\n",
    "y_val = val_labels.tolist()\n",
    "\n",
    "# Start training the model\n",
    "train_losses, val_loss = train(model, optimizer, loss_fn, train_dataloader, val_dataloader, y_val, epochs, batchSize)\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(val_loss, label='test loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Saving the trained model weights to disk\n",
    "PATH = 'custom-Transformer-Mulimodal.pth'\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the positional encoding used in the Transformer model.\n",
    "    The code is inspired from PyTorch's tutorial on Transformers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, vocab_size=5000, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the PositionalEncoding module.\n",
    "\n",
    "        Parameters:\n",
    "            d_model (int): Dimension of the model\n",
    "            vocab_size (int): Size of the vocabulary\n",
    "            dropout (float): Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize dropout layer with given dropout rate\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Initialize positional encoding matrix with zeros\n",
    "        pe = torch.zeros(vocab_size, d_model)\n",
    "\n",
    "        # Generate positions from 0 to vocab_size - 1\n",
    "        position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Calculate the terms to be divided for sine and cosine\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float()\n",
    "            * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        # Generate sine and cosine positional encodings\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add an extra dimension for batch size\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # Register the tensor 'pe' as a buffer to be part of the model\n",
    "        # It will not be updated during backpropagation\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for positional encoding.\n",
    "        Parameters:\n",
    "            x (Tensor): The input sequence of shape (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            Tensor: Positionally-encoded input\n",
    "        \"\"\"\n",
    "        # Add the positional encoding to the input tensor 'x'\n",
    "        # Note: ': x.size(1)' is used to match the sequence length of 'x'\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "\n",
    "        # Apply dropout for regularization\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Multi_TF_Class (nn.Module):\n",
    "    \"\"\"\n",
    "    Text classifier that uses a Transformer encoder along with static features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embedding,\n",
    "                 nhead=10,\n",
    "                 dim_feedforward=512,\n",
    "                 num_layers=6,\n",
    "                 dropout=0.1,\n",
    "                 classifier_dropout=0.1,\n",
    "                 n_node_layer1=160,\n",
    "                 n_node_layer2=32,\n",
    "                 static_size=75):\n",
    "        \"\"\"\n",
    "        Initialize the model.\n",
    "\n",
    "        Parameters:\n",
    "            embedding (Tensor): Pre-trained word embeddings\n",
    "            nhead (int): Number of attention heads\n",
    "            dim_feedforward (int): Dimension of the feedforward network\n",
    "            num_layers (int): Number of transformer layers\n",
    "            dropout (float): Dropout rate for transformer\n",
    "            classifier_dropout (float): Dropout rate for classifier layers\n",
    "            n_node_layer1 (int): Number of nodes in the first hidden layer\n",
    "            n_node_layer2 (int): Number of nodes in the second hidden layer\n",
    "            static_size (int): Size of the static feature vector\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Multi_TF_Class, self).__init__()\n",
    "\n",
    "        vocab_size, d_model = embedding.shape\n",
    "        \n",
    "        # Ensure that the number of heads divides evenly into the model dimension\n",
    "        assert d_model % nhead == 0, \"nheads must divide evenly into d_model\"\n",
    "\n",
    "        # Embedding Layer\n",
    "        self.emb = nn.Embedding.from_pretrained(embedding, freeze=False, padding_idx=0)\n",
    "        \n",
    "        # Positional Encoding Layer\n",
    "        self.pos_encoder = PositionalEncoding(\n",
    "            d_model=d_model,\n",
    "            dropout=dropout,\n",
    "            vocab_size=vocab_size,\n",
    "        )\n",
    "        \n",
    "        # Transformer Encoder Layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        \n",
    "        # Static feature settings\n",
    "        self.static_size = static_size\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Batch Normalization Layers\n",
    "        self.batchnorm1 = nn.BatchNorm1d(n_node_layer1, momentum=0.1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(n_node_layer2, momentum=0.1)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.linear1 = nn.Linear(d_model + static_size, n_node_layer1)\n",
    "        self.linear2 = nn.Linear(n_node_layer1, n_node_layer2)\n",
    "        \n",
    "        # Activation and Dropout Layers\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=classifier_dropout)\n",
    "        \n",
    "        # Final Classifier Layer\n",
    "        self.classifier = nn.Linear(n_node_layer2, 3)\n",
    "\n",
    "    def forward(self, x, x_static):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tensor): Textual input sequences\n",
    "            x_static (Tensor): Static features\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Classifier output\n",
    "        \"\"\"\n",
    "\n",
    "        # Embedding and Positional Encoding\n",
    "        x = self.emb(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # Take the mean over the sequence dimension\n",
    "        TF_out = x.mean(dim=1)\n",
    "        \n",
    "        # Concatenate the static features\n",
    "        inputs = torch.cat([TF_out, x_static], dim=1)\n",
    "        \n",
    "        # Fully Connected Layers with activations, batch normalization, and dropout\n",
    "        out = self.linear1(inputs)\n",
    "        out = self.relu(out)\n",
    "        out = self.batchnorm1(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.linear2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.batchnorm2(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Classifier\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Convert the pretrained weight array to a PyTorch tensor\n",
    "# Dimension of pretrained_weight = 50\n",
    "pretrained_weight = torch.FloatTensor(pretrained_weight)\n",
    "\n",
    "# Instantiate the model with the various hyperparameters and the pretrained weights\n",
    "model = Multi_TF_Class(\n",
    "    pretrained_weight,       # pretrained embeddings\n",
    "    nhead=10,                # number of heads in multihead attention\n",
    "    dim_feedforward=768,     # dimensions of the feedforward layers in the transformer encoder\n",
    "    num_layers=12,           # number of transformer encoder layers\n",
    "    dropout=0.1,             # dropout rate for the transformer encoder\n",
    "    classifier_dropout=0.5,  # dropout rate for the classifier\n",
    "    n_node_layer1=256,       # number of nodes in the first hidden layer\n",
    "    n_node_layer2=48,        # number of nodes in the second hidden layer\n",
    "    static_size=75,          # number of static features\n",
    ")\n",
    "\n",
    "# Move the model to the selected device (GPU or CPU)\n",
    "# This ensures that all the computations will be carried out on the same device as the model\n",
    "model.to(device)\n",
    "\n",
    "def get_accuracy(out, actual_labels, batchSize):\n",
    "    '''\n",
    "    Computes the accuracy of a model's predictions for a given batch.\n",
    "    \n",
    "    Parameters:\n",
    "    - out (Tensor): The log probabilities or logits returned by the model.\n",
    "    - actual_labels (Tensor): The actual labels for the batch.\n",
    "    - batchSize (int): The size of the batch.\n",
    "    \n",
    "    Returns:\n",
    "    float: The accuracy for the batch.\n",
    "    '''\n",
    "    # Get the predicted labels from the maximum value of log probabilities\n",
    "    predictions = out.max(dim=1)[1]\n",
    "    # Count the number of correct predictions\n",
    "    correct = (predictions == actual_labels).sum().item()\n",
    "    # Compute the accuracy for the batch\n",
    "    accuracy = correct / batchSize\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Define the training function\n",
    "def train(model, optimizer, loss_fn, train_dataloader, val_dataloader, y_val, epochs=20, batchSize=16):\n",
    "    \"\"\"\n",
    "    Train a PyTorch model using the given parameters.\n",
    "\n",
    "    Parameters:\n",
    "        model (nn.Module): The PyTorch model to train.\n",
    "        optimizer (Optimizer): The optimizer to use during training.\n",
    "        loss_fn (function): The loss function.\n",
    "        train_dataloader (DataLoader): DataLoader for the training set.\n",
    "        val_dataloader (DataLoader): DataLoader for the validation set.\n",
    "        y_val (array-like): Ground truth labels for the validation set.\n",
    "        epochs (int, optional): Number of epochs to train for. Default is 20.\n",
    "        batchSize (int, optional): Size of batches. Default is 16.\n",
    "\n",
    "    Returns:\n",
    "        train_losses (array): Array of training losses for each epoch.\n",
    "        val_losses (array): Array of validation losses for each epoch.\n",
    "    \"\"\"\n",
    "    # Initialize device based on GPU/CPU availability\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialize variables to keep track of best metrics\n",
    "    best_val_loss = 2\n",
    "    best_accuracy = 0\n",
    "    best_AUC = 0\n",
    "    best_p = []\n",
    "    \n",
    "    # Initialize arrays to store losses for plotting later\n",
    "    train_losses = np.zeros(epochs)\n",
    "    val_losses = np.zeros(epochs)\n",
    "\n",
    "    # Start of the training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Train Acc':^9} | {'Val Loss':^10} | {'Val Acc':^9} | {'Val AUC':^9} | {'Val F1':^9} |{'Elapsed':^9}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    # Loop through each epoch\n",
    "    for epoch_i in tqdm(range(epochs)):\n",
    "        # Keep track of time taken for each epoch\n",
    "        t0_epoch = time.time()\n",
    "        total_loss = 0\n",
    "        epoc_acc = 0\n",
    "        \n",
    "        # Set model to train mode\n",
    "        model.train()\n",
    "\n",
    "        # Loop through each batch in the training dataloader\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Move the batch tensors to the device\n",
    "            b_input_ids, b_input_tbl, b_labels = batch\n",
    "            b_input_ids, b_input_tbl, b_labels = b_input_ids.to(device), b_input_tbl.to(device), b_labels.long().to(device)\n",
    "            \n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass\n",
    "            logits = model(b_input_ids, b_input_tbl)\n",
    "            logits = logits.float().to(device)\n",
    "            \n",
    "            # Compute the loss and accumulate it\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Compute the accuracy for the batch and accumulate it\n",
    "            epoc_acc += get_accuracy(logits, b_labels, batchSize)\n",
    "            \n",
    "            # Perform a backward pass to update the weights\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the optimizer parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Compute the average training loss and accuracy for the epoch\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        avg_train_acc = epoc_acc / len(train_dataloader)\n",
    "        \n",
    "        # Store the average training loss\n",
    "        train_losses[epoch_i] = avg_train_loss\n",
    "\n",
    "        # If a validation dataloader is provided, perform evaluation\n",
    "        if val_dataloader is not None:\n",
    "            val_loss, val_accuracy, val_AUC, val_f1, p = evaluate(model, val_dataloader, y_val, batchSize=16)\n",
    "            \n",
    "            # Store the validation loss\n",
    "            val_losses[epoch_i] = val_loss\n",
    "\n",
    "            # Update best metrics if necessary\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "            if val_AUC > best_AUC:\n",
    "                best_AUC = val_AUC\n",
    "                best_p = p\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_wts_BERTv1 = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "            # Calculate time taken for the epoch\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            # Print all metrics\n",
    "            print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {avg_train_acc:^9.2f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {val_AUC:^9.4f} | {val_f1:^9.4f} | {time_elapsed:^9.2f}\")\n",
    "            \n",
    "    # Training complete, print the best metrics\n",
    "    print(f\"\\nTraining complete! Best accuracy: {best_accuracy:.2f}%.\")\n",
    "    print(f\"Training complete! Best AUC: {best_AUC:.4f}.\")\n",
    "    \n",
    "    # Return training and validation losses and best model\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader, y_val, batchSize=16):\n",
    "    \"\"\"\n",
    "    Evaluate a PyTorch model on a validation set.\n",
    "\n",
    "    Parameters:\n",
    "        model (nn.Module): The PyTorch model to evaluate.\n",
    "        val_dataloader (DataLoader): DataLoader for the validation set.\n",
    "        y_val (array-like): Ground truth labels for the validation set.\n",
    "        batchSize (int, optional): Size of batches. Default is 16.\n",
    "\n",
    "    Returns:\n",
    "        val_loss (float): Average validation loss.\n",
    "        val_accuracy (float): Average validation accuracy.\n",
    "        val_AUC (float): Area under the ROC curve for the validation set.\n",
    "        val_f1 (float): F1 score for the validation set.\n",
    "        p (array): Array of prediction probabilities.\n",
    "\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize variables to store evaluation metrics and predictions\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    outputs_list = []\n",
    "    y_pred_list = []\n",
    "    probs_list = []\n",
    "    \n",
    "    preds_list = torch.tensor([], dtype=torch.long, device=device)\n",
    "    labels_list = torch.tensor([], dtype=torch.long, device=device)\n",
    "\n",
    "    # Loop over batches in the validation DataLoader\n",
    "    for batch in val_dataloader:\n",
    "        # Extract input features and labels from the batch\n",
    "        b_input_ids, b_input_tbl, b_labels = batch\n",
    "        b_input_ids, b_input_tbl, b_labels = b_input_ids.to(device),  b_input_tbl.to(device), b_labels.long().to(device)\n",
    "\n",
    "        # Perform forward pass and compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_input_tbl)\n",
    "            logits = logits.float().to(device)\n",
    "            \n",
    "        # Calculate probabilities using softmax\n",
    "        y_val_probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "        outputs_list.append(logits)\n",
    "        probs_list.append(y_val_probs)\n",
    "        \n",
    "        # Get the predicted labels\n",
    "        predictions = logits.max(dim=1)[1]\n",
    "        preds_list = torch.cat([preds_list, predictions])\n",
    "        labels_list = torch.cat([labels_list, b_labels])\n",
    "\n",
    "        # Calculate loss for the batch\n",
    "        loss = loss_fn(logits, b_labels).to(device)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Calculate accuracy for the batch\n",
    "        val_accuracy = get_accuracy(logits, b_labels, batchSize)\n",
    "\n",
    "    # Compute average loss and accuracy\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "    \n",
    "    # Convert tensors to NumPy arrays for scikit-learn metrics\n",
    "    p = torch.cat(probs_list).detach().cpu().numpy()\n",
    "    preds_list = preds_list.cpu().numpy()\n",
    "    labels_list = labels_list.cpu().numpy()\n",
    "\n",
    "    # Calculate F1 score and AUC\n",
    "    val_f1 = f1_score(labels_list, preds_list, average='weighted')\n",
    "    val_AUC = roc_auc_score(y_val, p, multi_class='ovr')\n",
    "    \n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return val_loss, val_accuracy, val_AUC, val_f1, p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1dc3d0-82c4-45aa-a587-092f3a3e3c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This following code performs the following operations:\n",
    "\n",
    "1. Load the pre-trained model from the specified file.\n",
    "2. Evaluate the model on a test set to generate predictions.\n",
    "3. Compute and plot Receiver Operating Characteristic (ROC) curves for each class and their macro-average.\n",
    "4. Compute and plot Precision-Recall (P-R) curves for each class and their macro-average.\n",
    "5. Print out the classification report and the confusion matrix for model evaluation.\n",
    "\n",
    "Outputs:\n",
    "- Plots of ROC and P-R curves.\n",
    "- Printed classification report and confusion matrix.\n",
    "\"\"\"\n",
    "\n",
    "# Define the path where the model's state dictionary will be saved\n",
    "PATH = 'custom-Transformer-Mulimodal.pth'\n",
    "# Save the model's state dictionary to the specified path\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('Multi_CustomT.pth'))\n",
    "\n",
    "# Get predictions for test set\n",
    "model.eval()\n",
    "test_probabilities = []\n",
    "test_true_labels = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_tbl = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(b_input_ids, b_input_tbl)\n",
    "        logits = logits.float().to(device)\n",
    "    \n",
    "    #logits = outputs[0]\n",
    "    probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "    test_probabilities.extend(probs.detach().cpu().numpy())\n",
    "    test_true_labels.extend(b_labels.detach().cpu().numpy())\n",
    "\n",
    "test_probabilities = np.array(test_probabilities)\n",
    "test_true_labels = np.array(test_true_labels)\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "all_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "for i in range(3):\n",
    "    fpr[i], tpr[i], _ = roc_curve(test_true_labels == i, test_probabilities[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    plt.plot(fpr[i], tpr[i], lw=2, label='ROC curve of class {0} (area = {1:0.4f})'.format(i, roc_auc[i]))\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(3):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "mean_tpr /= 3\n",
    "mean_auc = auc(all_fpr, mean_tpr)\n",
    "plt.plot(all_fpr, mean_tpr, color='b', linestyle='--', lw=2, label='Macro-average ROC (area = {0:0.4f})'.format(mean_auc))\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "plt.show()\n",
    "\n",
    "# Compute macro-average P-R curve and P-R area\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "\n",
    "for i in range(3):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(test_true_labels == i, test_probabilities[:, i])\n",
    "    average_precision[i] = average_precision_score(test_true_labels == i, test_probabilities[:, i])\n",
    "    plt.step(recall[i], precision[i], lw=2, where='post', label='P-R curve of class {0} (area = {1:0.4f})'.format(i, average_precision[i]))\n",
    "\n",
    "# Macro-average P-R curve and P-R area\n",
    "mean_precision = sum(average_precision.values()) / 3\n",
    "plt.plot([0, 1], [mean_precision, mean_precision], linestyle='--', lw=2, color='b', label='Macro-average P-R (area = {0:0.4f})'.format(mean_precision))\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Precision-Recall (P-R) Curves')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "predicted_classes = np.argmax(test_probabilities, axis=1)\n",
    "print(classification_report(test_true_labels, predicted_classes, target_names=['Success', 'Low IOP', 'High IOP']))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(test_true_labels, predicted_classes))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
